# -*- coding: utf-8 -*-
"""Gradient_blend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PSggp8MqrHH_oDhX8f4Oy8GOlv5i0V9C

To use: call function `train_gradient_blend(unimodal_models,  multimodal_classification_head, unimodal_classification_heads, train_data, num_epoch, gb_epoch,  batch_size, v_rate,lr, weight_decay=0.0, optimtype=torch.optim.SGD, AUPRC=False)`

**unimodal_models:** this input needs to be a list of unimodal models, each must be a pytorch nn.Module, the forward function of the ith model must take in exactly **one** argument as input data (batch_size x input_dim_i), and outputs (batch_size x representation_dim_i)

**multimodal_classification_head:** this input needs to bea pytorch nn.Module, its forward function must take in exactly **one** argument as input data of size (batch_size x total_representation_dim), where total_representation_dim is the sum of representation_dim_i for all i. Its input will be a concatenation of all unimodal representation vectors produced by unimodal models in the same order of **unimodal_models**. Its output should have size (batch_size x num_classes)

**unimodal_classification_heads:** this input needs to be a list of unimodal models, each must be a pytorch nn.Module, the forward function of the ith model must take in exactly **one** argument as input data (batch_size x representation_dim_i) and output (batch_size x num_classes)

**train_datas:** training data as a list of tuples, where each tuple is (x1,x2,...xm,y) where x1,x2,...xm is input to each of the m modalities and y is the classification label. Each input must be a torch tensor of size input_dim_i.

**num_epoch:** int, total number of epochs to train. must be integer multiple of gb_epoch

**gb_epoch:** int, the number of epochs to estimate gradient blend weight

**batch_size:** int, batch size

**v_rate:** float between 0 and 1, the percentage of training data used as valid set when doing GB estimation

**lr:** float, learning rate

**weight_decay:** float, l2 regularization, default to 0

**optimtype:** type of optimizer to use, default is optim.SGD

**AUPRC:** boolean, whether to compute AUPRC or not. the computed value only makes sense for binary classifications. Default is False.
"""

import torch
from torch import nn
import copy
import random
from torch.utils.data import DataLoader

criterion=nn.CrossEntropyLoss()

def getloss(model,head,data,monum,batch_size):
  losses=0.0
  total=0
  with torch.no_grad():
    for j in data:
      total += len(j[0])
      train_x=j[monum].cuda()
      train_y=j[-1].cuda()
      out=head(model(train_x))
      loss=criterion(out,train_y.squeeze())
      #print(len(j[0]))
      losses+=loss*len(j[0])
  return losses/total

def train_unimodal(model,head,optim,trains,valids,monum,epoch,batch_size):

  ltN=getloss(model,head,trains,monum,batch_size)
  lvN=getloss(model,head,valids,monum,batch_size)
  for i in range(epoch): 
    totalloss=0.0
    total=0
    for j in trains:
      total += len(j[0])
      train_x=j[monum].cuda()
      train_y=j[-1].cuda()
      optim.zero_grad()
      out=head(model(train_x))
      loss=criterion(out,train_y.squeeze())
      totalloss += loss * len(j[0])
      loss.backward()
      optim.step()
    print("Epoch "+str(i)+" loss: "+str(totalloss/ total))

  ltNn=getloss(model,head,trains,monum,batch_size)
  lvNn=getloss(model,head,valids,monum,batch_size)
  print("Final train loss: "+str(ltNn)+" valid loss: "+str(lvNn))
  oNn=lvNn-ltNn
  oN=lvN-ltN
  oi=oNn-oN
  g=lvNn-lvN
  #oi=oNn
  #g=lvNn
  print("raw: "+str(g/(oi*oi)))
  return abs(g/(oi*oi))

def multimodalcondense(models,train_x):
  outs=multimodalcompute(models,train_x)
  return torch.cat(outs,dim=1)

def multimodalcompute(models,train_x):
  outs=[]
  for i in range(len(models)):
    outs.append(models[i](train_x[i]))
  return outs

def getmloss(models,head,data,batch_size):
  losses=0.0
  total=0
  with torch.no_grad():
    for j in data:
      total += len(j[0])
      train_x=[x.cuda() for x in j[:-1]]
      train_y=j[-1].cuda()
      out=head(multimodalcondense(models,train_x))
      loss=criterion(out,train_y.squeeze())
      losses+=loss*len(j[0])
  return losses/float(total)

def train_multimodal(models,head,optim,trains,valids,epoch,batch_size):
  ltN=getmloss(models,head,trains,batch_size)
  lvN=getmloss(models,head,valids,batch_size)
  for i in range(epoch):   
    totalloss=0.0
    total=0
    for j in trains:
      total += len(j[0])
      train_x=[x.cuda() for x in j[:-1]]
      train_y=j[-1].cuda()
      optim.zero_grad()
      out=head(multimodalcondense(models,train_x))
      loss=criterion(out,train_y.squeeze())
      totalloss += loss*len(j[0])
      loss.backward()
      optim.step()
    print("Epoch "+str(i)+" loss: "+str(totalloss/total) )
  ltNn=getmloss(models,head,trains,batch_size)
  lvNn=getmloss(models,head,valids,batch_size)
  print("Final train loss: "+str(ltNn)+" valid loss: "+str(lvNn))
  oNn=lvNn-ltNn
  oN=lvN-ltN
  oi=oNn-oN
  g=lvNn-lvN
  #oi=oNn
  #g=lvNn
  print("raw: "+str(g/(oi*oi)))
  return abs(g/(oi*oi))


def gb_estimate(unimodal_models,multimodal_classification_head, unimodal_classification_heads,train_dataloader,gb_epoch, 
                batch_size, v_dataloader, lr, weight_decay=0.0, optimtype=torch.optim.SGD):
  weights=[]
  for i in range(len(unimodal_models)):
    print("At gb_estimate unimodal "+str(i))
    model=copy.deepcopy(unimodal_models[i]).cuda()
    head=copy.deepcopy(unimodal_classification_heads[i]).cuda()
    optim=optimtype(list(model.parameters())+list(head.parameters()),lr=lr,weight_decay=weight_decay)
    w=train_unimodal(model,head,optim,train_dataloader,v_dataloader,i,gb_epoch,batch_size)
    weights.append(w)
  print("At gb_estimate multimodal ")
  allcopies=[copy.deepcopy(x).cuda() for x in unimodal_models]
  mmcopy=copy.deepcopy(multimodal_classification_head).cuda()
  params=[]
  for model in allcopies:
    params.extend(list(model.parameters()))
  params.extend(list(mmcopy.parameters()))
  optim=optimtype(params,lr=lr,weight_decay=weight_decay)
  weights.append(train_multimodal(allcopies,mmcopy,optim,train_dataloader,v_dataloader,gb_epoch,batch_size))
  z=sum(weights)
  return [(w/z).item() for w in weights]
  
softmax=nn.Softmax()

import sklearn.metrics

def calcAUPRC(pts):
  true_labels=[int(x[1]) for x in pts]
  predicted_probs=[x[0] for x in pts]
  return sklearn.metrics.average_precision_score(true_labels, predicted_probs)

def train_gradient_blend(unimodal_models,  multimodal_classification_head, 
                                 unimodal_classification_heads, train_datas, valid_data, num_epoch, gb_epoch,  batch_size, v_rate,lr, weight_decay=0.0, optimtype=torch.optim.SGD, finetune_epoch=20,AUPRC=False):
  params=[]
  for model in unimodal_models:
    params.extend(model.parameters())
  for model in unimodal_classification_heads:
    params.extend(model.parameters())
  params.extend(multimodal_classification_head.parameters())
  optim=optimtype(params,lr=lr,weight_decay=weight_decay)
  splitloc=int(len(train_datas)*v_rate)
  v_data = train_datas[0:splitloc]
  train_data = train_datas[splitloc:]
  train_dataloader = DataLoader(train_data,shuffle=True,num_workers=8,batch_size=batch_size);
  tv_dataloader = DataLoader(v_data,shuffle=False,num_workers=8,batch_size=batch_size);
  valid_dataloader = DataLoader(valid_data,shuffle=False,num_workers=8,batch_size=batch_size);
  finetunehead=copy.deepcopy(multimodal_classification_head)
  optimi=optimtype(finetunehead.parameters(),lr=lr,weight_decay=weight_decay)
  for i in range(num_epoch//gb_epoch):
    #"""
    weights=gb_estimate(unimodal_models,  multimodal_classification_head, 
                        unimodal_classification_heads, train_dataloader, gb_epoch, batch_size,tv_dataloader, lr, weight_decay, optimtype)
    #"""
    #weights=(1.0,1.0,1.0)
    print("epoch "+str(i*gb_epoch)+" weights: "+str(weights))
    for jj in range(gb_epoch):
      totalloss=0.0
      for j in train_dataloader:
        train_x=[x.cuda() for x in j[:-1]]
        train_y=j[-1].cuda()
        optim.zero_grad()
        outs=multimodalcompute(unimodal_models,train_x)
        catout=torch.cat(outs,dim=1)
        blendloss=criterion(multimodal_classification_head(catout),train_y.squeeze())*weights[-1]
        for ii in range(len(unimodal_models)):
          loss=criterion(unimodal_classification_heads[ii](outs[ii]),train_y.squeeze())
          blendloss += loss * weights[ii]
        totalloss += blendloss*len(j[0])
        blendloss.backward()
        optim.step()
      print("epoch "+str(jj+i*gb_epoch)+" blend train loss: "+str(totalloss/len(train_data)))
    #finetunes classification head
    finetunetrains=[]
    with torch.no_grad():
      for j in train_dataloader:
        train_x=[x.cuda() for x in j[:-1]]
        train_y=j[-1].cuda()
        outs=multimodalcompute(unimodal_models,train_x)
        catout=torch.cat(outs,dim=1)
        for iii in range(len(catout)):
          finetunetrains.append([catout[iii].cpu(),train_y[iii].cpu()])
    ftt_dataloader = DataLoader(finetunetrains,shuffle=True,num_workers=8,batch_size=batch_size)
    for jj in range(finetune_epoch):
      totalloss=0.0
      for j in ftt_dataloader:
        optimi.zero_grad()
        train_x=j[0].cuda()
        train_y=j[-1].cuda()
        blendloss=criterion(finetunehead(train_x),train_y.squeeze())
        totalloss += blendloss * len(j[0])
        blendloss.backward()
        optimi.step()
      print("finetune train loss: "+str(totalloss/len(train_data)))
    with torch.no_grad():
      totalloss=0.0
      total=0
      corrects=0
      auprclist=[]
      for j in valid_dataloader:
        valid_x=[x.cuda() for x in j[:-1]]
        valid_y=j[-1].cuda()
        outs=multimodalcompute(unimodal_models,valid_x)
        catout=torch.cat(outs,dim=1)
        predicts=finetunehead(catout)
        blendloss=criterion(predicts,valid_y.squeeze())
        totalloss += blendloss*len(j[0])
        predictlist=predicts.tolist()
        for ii in range(len(j)):
          total+=1
          if AUPRC:
            predictval=softmax(predicts[ii])
            auprclist.append((predictval[1].item(),valid_y[ii].item()))
          if predictlist[ii].index(max(predictlist[ii]))==valid_y[ii]:
            corrects+=1
      print("epoch "+str((i+1)*gb_epoch-1)+" valid loss: "+str(totalloss/total)+" acc: "+str(float(corrects)/total))
      if AUPRC:
        print("With AUPRC: "+str(calcAUPRC(auprclist)))

